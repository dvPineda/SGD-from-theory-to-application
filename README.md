# SGD-from-theory-to-application
A comprehensive study of Gradient Descent (GD) and Stochastic Gradient Descent (SGD) algorithms from its mathematical theory to its application on real datasets 

## Acknowledgements
- For an insightful comparison between Backpropagation and Stochastic Gradient Descent, see [Difference Between Backpropagation and Stochastic Gradient Descent](https://machinelearningmastery.com/difference-between-backpropagation-and-stochastic-gradient-descent/).
- An overview of the cost function and gradient descent can be found at [Machine Learning: Cost Function and Gradient Descent](https://towardsdatascience.com/machine-leaning-cost-function-and-gradient-descend-75821535b2ef).
- For understanding Mini-Batch Gradient Descent with Python, visit [ML | Mini-Batch Gradient Descent with Python](https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/).
- A detailed implementation of Gradient Descent in Python is explained in [Implementing Gradient Descent in Python from Scratch](https://towardsdatascience.com/implementing-gradient-descent-in-python-from-scratch-760a8556c31f).
- The mathematical background of Gradient Descent is well described in [Mathematics Behind Gradient Descent Simply Explained](https://medium.com/nerd-for-tech/mathematics-behind-gradient-descent-simply-explained-c9a17698fd6).
- For a guide on Gradient Descent Algorithm in Python, particularly Stochastic Gradient Descent, refer to [Gradient Descent Algorithm in Python](https://realpython.com/gradient-descent-algorithm-python/#stochastic-gradient-descent-algorithms).


